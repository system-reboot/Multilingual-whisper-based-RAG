{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Prepare environment"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:47:18.448577Z","iopub.status.busy":"2023-11-19T13:47:18.448233Z","iopub.status.idle":"2023-11-19T13:48:22.048288Z","shell.execute_reply":"2023-11-19T13:48:22.047133Z","shell.execute_reply.started":"2023-11-19T13:47:18.448551Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.2.1)\n","Collecting pip\n","  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/47/6a/453160888fab7c6a432a6e25f8afe6256d0d9f2cbd25971021da6491d899/pip-23.3.1-py3-none-any.whl.metadata\n","  Downloading pip-23.3.1-py3-none-any.whl.metadata (3.5 kB)\n","Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 23.2.1\n","    Uninstalling pip-23.2.1:\n","      Successfully uninstalled pip-23.2.1\n","Successfully installed pip-23.3.1\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Collecting datasets\n","  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.0)\n","Collecting transformers\n","  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.24.1)\n","Requirement already satisfied: soundfile in /opt/conda/lib/python3.10/site-packages (0.12.1)\n","Requirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.10.1)\n","Collecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n","Collecting jiwer\n","  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.13.0)\n","Collecting tensorboard\n","  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n","Collecting gradio\n","  Downloading gradio-4.4.1-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\n","Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Downloading pyarrow_hotfix-0.5-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\n","Collecting huggingface-hub>=0.18.0 (from datasets)\n","  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\n","Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.15.1)\n","Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.0.1)\n","Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.11.3)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.3.2)\n","Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\n","Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.57.1)\n","Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.8.0)\n","Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.5.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.5)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\n","Requirement already satisfied: rapidfuzz<4,>=3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (3.5.2)\n","Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.22.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.4.4)\n","Requirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (68.1.2)\n","Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.1.2)\n","Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.101.1)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting gradio-client==0.7.0 (from gradio)\n","  Downloading gradio_client-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting httpx (from gradio)\n","  Downloading httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.13.0)\n","Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\n","Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.3)\n","Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.3)\n","Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.5)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.1.0)\n","Collecting pydantic>=2.0 (from gradio)\n","  Downloading pydantic-2.5.1-py3-none-any.whl.metadata (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\n","Collecting python-multipart (from gradio)\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.9 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.9.0)\n","Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.23.2)\n","Collecting websockets<12.0,>=10.0 (from gradio-client==0.7.0->gradio)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n","Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n","Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.21)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.15)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.40.1)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa) (4.0.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n","Collecting pydantic-core==2.14.3 (from pydantic>=2.0->gradio)\n","  Downloading pydantic_core-2.14.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n","Collecting typing-extensions>=4.1.1 (from librosa)\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n","INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n","Collecting tokenizers<0.19,>=0.14 (from transformers)\n","  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n","Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n","Requirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (13.5.2)\n","Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n","Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (0.27.0)\n","Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (3.7.1)\n","Collecting httpcore (from httpx->gradio)\n","  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.9.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n","Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->gradio) (1.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.0)\n","Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n","Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading gradio-4.4.1-py3-none-any.whl (15.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic-2.5.1-py3-none-any.whl (381 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.6/381.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_core-2.14.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n","Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=51b52ae9fc9253695c9198f37759b13ca8b81b234398cb78af5e5b7c062be134\n","  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n","Successfully built ffmpy\n","Installing collected packages: ffmpy, websockets, typing-extensions, tomlkit, semantic-version, python-multipart, pyarrow-hotfix, jiwer, httpcore, aiofiles, pydantic-core, huggingface-hub, httpx, tokenizers, pydantic, gradio-client, transformers, tensorboard, datasets, gradio, evaluate\n","  Attempting uninstall: websockets\n","    Found existing installation: websockets 12.0\n","    Uninstalling websockets-12.0:\n","      Successfully uninstalled websockets-12.0\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: tomlkit\n","    Found existing installation: tomlkit 0.12.2\n","    Uninstalling tomlkit-0.12.2:\n","      Successfully uninstalled tomlkit-0.12.2\n","  Attempting uninstall: pydantic-core\n","    Found existing installation: pydantic_core 2.10.1\n","    Uninstalling pydantic_core-2.10.1:\n","      Successfully uninstalled pydantic_core-2.10.1\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.17.3\n","    Uninstalling huggingface-hub-0.17.3:\n","      Successfully uninstalled huggingface-hub-0.17.3\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.14.1\n","    Uninstalling tokenizers-0.14.1:\n","      Successfully uninstalled tokenizers-0.14.1\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.10.12\n","    Uninstalling pydantic-1.10.12:\n","      Successfully uninstalled pydantic-1.10.12\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.0\n","    Uninstalling transformers-4.35.0:\n","      Successfully uninstalled transformers-4.35.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.13.0\n","    Uninstalling tensorboard-2.13.0:\n","      Successfully uninstalled tensorboard-2.13.0\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.1.0\n","    Uninstalling datasets-2.1.0:\n","      Successfully uninstalled datasets-2.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\n","cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\n","dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n","jupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.0.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.0.0 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\n","pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\n","pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.3 which is incompatible.\n","tensorflow 2.13.0 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.15.1 which is incompatible.\n","tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\n","tensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\n","tensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\n","ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\n","ydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiofiles-23.2.1 datasets-2.15.0 evaluate-0.4.1 ffmpy-0.3.1 gradio-4.4.1 gradio-client-0.7.0 httpcore-1.0.2 httpx-0.25.1 huggingface-hub-0.19.4 jiwer-3.0.3 pyarrow-hotfix-0.5 pydantic-2.4.2 pydantic-core-2.14.3 python-multipart-0.0.6 semantic-version-2.10.0 tensorboard-2.15.1 tokenizers-0.15.0 tomlkit-0.12.0 transformers-4.35.2 typing-extensions-4.7.1 websockets-11.0.3\n"]}],"source":["!pip install --upgrade pip\n","!pip install --upgrade datasets transformers accelerate soundfile librosa evaluate jiwer tensorboard gradio"]},{"cell_type":"markdown","metadata":{},"source":["# Loading the dataset"]},{"cell_type":"markdown","metadata":{},"source":["Common Voice is a series of crowd-sourced datasets where speakers record text from Wikipedia in various languages present in Hugging Face Dataset repositories. Common Voice 11.0 contains approximately 12 hours of labelled Hindi data, 4 of which are held-out test data."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:48:22.050763Z","iopub.status.busy":"2023-11-19T13:48:22.050445Z","iopub.status.idle":"2023-11-19T13:48:22.677514Z","shell.execute_reply":"2023-11-19T13:48:22.676507Z","shell.execute_reply.started":"2023-11-19T13:48:22.050734Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","login(token='xxxxxxxxxxxxxxxxxxxxx')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:48:22.679247Z","iopub.status.busy":"2023-11-19T13:48:22.678840Z","iopub.status.idle":"2023-11-19T13:48:50.013647Z","shell.execute_reply":"2023-11-19T13:48:50.012647Z","shell.execute_reply.started":"2023-11-19T13:48:22.679214Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset, DatasetDict\n","\n","common_voice = DatasetDict()\n","\n","common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train+validation\", token=True)\n","common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test\", token=True)\n","\n","print(common_voice)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:48:50.016160Z","iopub.status.busy":"2023-11-19T13:48:50.015887Z","iopub.status.idle":"2023-11-19T13:48:50.028357Z","shell.execute_reply":"2023-11-19T13:48:50.027465Z","shell.execute_reply.started":"2023-11-19T13:48:50.016135Z"},"trusted":true},"outputs":[],"source":["#Removing unwanted columns\n","common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])"]},{"cell_type":"markdown","metadata":{},"source":["# Loading Whisper feature extractor"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:48:50.031022Z","iopub.status.busy":"2023-11-19T13:48:50.030411Z","iopub.status.idle":"2023-11-19T13:48:55.145981Z","shell.execute_reply":"2023-11-19T13:48:55.145098Z","shell.execute_reply.started":"2023-11-19T13:48:50.030989Z"},"trusted":true},"outputs":[],"source":["from transformers import WhisperFeatureExtractor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")"]},{"cell_type":"markdown","metadata":{},"source":["# Loading Whisper Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:48:55.147752Z","iopub.status.busy":"2023-11-19T13:48:55.147278Z","iopub.status.idle":"2023-11-19T13:48:56.893596Z","shell.execute_reply":"2023-11-19T13:48:56.892565Z","shell.execute_reply.started":"2023-11-19T13:48:55.147724Z"},"trusted":true},"outputs":[],"source":["from transformers import WhisperTokenizer\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"Hindi\", task=\"transcribe\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:48:56.895306Z","iopub.status.busy":"2023-11-19T13:48:56.894974Z","iopub.status.idle":"2023-11-19T13:49:16.701879Z","shell.execute_reply":"2023-11-19T13:49:16.701021Z","shell.execute_reply.started":"2023-11-19T13:48:56.895276Z"},"trusted":true},"outputs":[],"source":["input_str = common_voice[\"train\"][0][\"sentence\"]\n","labels = tokenizer(input_str).input_ids\n","decoded_str = tokenizer.decode(labels, skip_special_tokens=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:49:16.703782Z","iopub.status.busy":"2023-11-19T13:49:16.703051Z","iopub.status.idle":"2023-11-19T13:49:16.709530Z","shell.execute_reply":"2023-11-19T13:49:16.708444Z","shell.execute_reply.started":"2023-11-19T13:49:16.703742Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input:                 हमने उसका जन्मदिन मनाया।\n","Decoded string:    हमने उसका जन्मदिन मनाया।\n"]}],"source":["print(f\"Input:                 {input_str}\")\n","print(f\"Decoded string:    {decoded_str}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Combine To Create A WhisperProcessor"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:49:16.711274Z","iopub.status.busy":"2023-11-19T13:49:16.710907Z","iopub.status.idle":"2023-11-19T13:49:17.105037Z","shell.execute_reply":"2023-11-19T13:49:17.104220Z","shell.execute_reply.started":"2023-11-19T13:49:16.711238Z"},"trusted":true},"outputs":[],"source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"Hindi\", task=\"transcribe\")"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:49:17.108441Z","iopub.status.busy":"2023-11-19T13:49:17.108141Z","iopub.status.idle":"2023-11-19T13:49:17.121938Z","shell.execute_reply":"2023-11-19T13:49:17.120847Z","shell.execute_reply.started":"2023-11-19T13:49:17.108414Z"},"trusted":true},"outputs":[],"source":["from datasets import Audio\n","common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:49:17.124014Z","iopub.status.busy":"2023-11-19T13:49:17.123654Z","iopub.status.idle":"2023-11-19T13:49:17.143862Z","shell.execute_reply":"2023-11-19T13:49:17.142975Z","shell.execute_reply.started":"2023-11-19T13:49:17.123986Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['audio', 'sentence'],\n","    num_rows: 6540\n","})\n"]}],"source":["print(common_voice['train'])"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:49:17.145136Z","iopub.status.busy":"2023-11-19T13:49:17.144872Z","iopub.status.idle":"2023-11-19T13:49:17.154333Z","shell.execute_reply":"2023-11-19T13:49:17.153412Z","shell.execute_reply.started":"2023-11-19T13:49:17.145113Z"},"trusted":true},"outputs":[],"source":["def prepare_dataset(batch):\n","    # load and resample audio data from 48 to 16kHz\n","    audio = batch[\"audio\"]\n","\n","    # compute log-Mel input features from input audio array \n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","\n","    # encode target text to label ids \n","    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n","    return batch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T13:49:17.155857Z","iopub.status.busy":"2023-11-19T13:49:17.155495Z","iopub.status.idle":"2023-11-19T14:01:40.486106Z","shell.execute_reply":"2023-11-19T14:01:40.484848Z","shell.execute_reply.started":"2023-11-19T13:49:17.155806Z"},"trusted":true},"outputs":[],"source":["common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)"]},{"cell_type":"markdown","metadata":{},"source":["# Defining a data collator"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T14:01:40.488606Z","iopub.status.busy":"2023-11-19T14:01:40.488210Z","iopub.status.idle":"2023-11-19T14:01:40.502595Z","shell.execute_reply":"2023-11-19T14:01:40.500040Z","shell.execute_reply.started":"2023-11-19T14:01:40.488565Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lengths and need different padding methods\n","        # first treat the audio inputs by simply returning torch tensors\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # get the tokenized label sequences\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # pad the labels to max length\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # if bos token is appended in previous tokenization step,\n","        # cut bos token here as it's append later anyways\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T14:01:40.504139Z","iopub.status.busy":"2023-11-19T14:01:40.503813Z","iopub.status.idle":"2023-11-19T14:01:40.983919Z","shell.execute_reply":"2023-11-19T14:01:40.982849Z","shell.execute_reply.started":"2023-11-19T14:01:40.504112Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"]},{"cell_type":"markdown","metadata":{},"source":["# Evalutaion "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T14:01:40.985627Z","iopub.status.busy":"2023-11-19T14:01:40.985240Z","iopub.status.idle":"2023-11-19T14:01:44.341486Z","shell.execute_reply":"2023-11-19T14:01:44.340599Z","shell.execute_reply.started":"2023-11-19T14:01:40.985590Z"},"trusted":true},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"wer\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T14:01:44.343273Z","iopub.status.busy":"2023-11-19T14:01:44.342956Z","iopub.status.idle":"2023-11-19T14:01:44.349582Z","shell.execute_reply":"2023-11-19T14:01:44.348596Z","shell.execute_reply.started":"2023-11-19T14:01:44.343245Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # replace -100 with the pad_token_id\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","\n","    # we do not want to group tokens when computing the metrics\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer}"]},{"cell_type":"markdown","metadata":{},"source":["# Loading a pre-trained checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T14:01:44.351147Z","iopub.status.busy":"2023-11-19T14:01:44.350817Z","iopub.status.idle":"2023-11-19T14:01:46.760629Z","shell.execute_reply":"2023-11-19T14:01:46.759581Z","shell.execute_reply.started":"2023-11-19T14:01:44.351120Z"},"trusted":true},"outputs":[],"source":["from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T14:01:46.762296Z","iopub.status.busy":"2023-11-19T14:01:46.761968Z","iopub.status.idle":"2023-11-19T14:01:46.767026Z","shell.execute_reply":"2023-11-19T14:01:46.766074Z","shell.execute_reply.started":"2023-11-19T14:01:46.762267Z"},"trusted":true},"outputs":[],"source":["model.config.forced_decoder_ids = None\n","model.config.suppress_tokens = []"]},{"cell_type":"markdown","metadata":{},"source":["# Defining Training argument"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T14:01:46.768599Z","iopub.status.busy":"2023-11-19T14:01:46.768275Z","iopub.status.idle":"2023-11-19T14:01:46.858670Z","shell.execute_reply":"2023-11-19T14:01:46.857851Z","shell.execute_reply.started":"2023-11-19T14:01:46.768565Z"},"trusted":true},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./whisper-tiny-hi\",  \n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n","    learning_rate=1e-5,\n","    warmup_steps=200,\n","    max_steps=1000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=200,\n","    save_steps=250,\n","    eval_steps=250,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=True,\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T14:01:46.860344Z","iopub.status.busy":"2023-11-19T14:01:46.859955Z","iopub.status.idle":"2023-11-19T14:01:52.579315Z","shell.execute_reply":"2023-11-19T14:01:52.578352Z","shell.execute_reply.started":"2023-11-19T14:01:46.860307Z"},"trusted":true},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=common_voice[\"train\"],\n","    eval_dataset=common_voice[\"test\"],\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-11-19T14:01:52.581144Z","iopub.status.busy":"2023-11-19T14:01:52.580510Z","iopub.status.idle":"2023-11-19T17:35:18.580878Z","shell.execute_reply":"2023-11-19T17:35:18.579869Z","shell.execute_reply.started":"2023-11-19T14:01:52.581113Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1000/1000 3:32:54, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Wer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>250</td>\n","      <td>0.506500</td>\n","      <td>0.654689</td>\n","      <td>85.232371</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.356500</td>\n","      <td>0.564379</td>\n","      <td>77.152290</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.289900</td>\n","      <td>0.534619</td>\n","      <td>76.504698</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.263700</td>\n","      <td>0.527111</td>\n","      <td>74.244476</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"]},{"data":{"text/plain":["TrainOutput(global_step=1000, training_loss=0.5798471503257752, metrics={'train_runtime': 12797.4399, 'train_samples_per_second': 2.501, 'train_steps_per_second': 0.078, 'total_flos': 7.858348130304e+17, 'train_loss': 0.5798471503257752, 'epoch': 4.88})"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["### Thus, the final validation loss was 0.527 and the Word Error Rate(WER) for whisper-tiny has been significantly downsampled from 141 to 74.24 for our particular hindi dataset usecase."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import WhisperForConditionalGeneration, WhisperProcessor\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"/checkpoints/checkpoint-1000\")\n","processor = WhisperProcessor.from_pretrained(\"/checkpoints/checkpoint-1000\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
